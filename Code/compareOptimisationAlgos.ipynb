{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "compareOptimisationAlgos.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO5HBRDXh8Z/bb38V5c7jAX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AdelaideUniversityMathSciences/MathsForAI/blob/main/Code/compareOptimisationAlgos.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "lLnFxeei0q5D"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code is borrowed from https://deeplearningmath.org/optimization-algorithms.html. \n",
        "\n",
        "- Run the code, to see how vanilla GD, GD with momentum, and GD with Nesterov momentum work on Rosenbrock's (non-convex) function.\n",
        "- The learning rate decays exponentially over time here. Switch it off. What happens to each method? Play with the learning rate.\n",
        "\n",
        "This code as written implements \"deterministic\" GD, rather than SGD. Let's change the problem to something we can apply the stochastic versions of these methods to, and also to something more resembling machine learning.\n",
        "\n",
        "Change the cost function to a the appropriate least-squares cost function for simple linear regression applied to the 3 data points (1,1), (2,2), (3,3). (Couldn't get much simpler).\n",
        "\n",
        "Then, change the function `grad(t)` to do simple online SGD (i.e., a minibatch size of 1). Mess around with all the parameters in the 'Initialization' cell until you get something that makes nice-looking plots. Compare the convergence of vanilla SGD, SGD with momentum, and Nesterov momentum."
      ],
      "metadata": {
        "id": "mUCA6Jeh74-3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "Vxty7uYa0kr1"
      },
      "outputs": [],
      "source": [
        "# Rosenbrock's banana function\n",
        "f = lambda t: (1 - t[0])**2 + 100*(t[1] - t[0]**2)**2\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def grad(t):\n",
        "    dx = -2*(1 - t[0]) - 4*100*t[0]*(t[1] - (t[0]**2))\n",
        "    dy = 2*100*(t[1] - (t[0]**2))\n",
        "\n",
        "    return np.array([dx, dy])\n",
        "\n",
        "def GradientDescent(f, grad, t, alpha, gamma, niter=100000):\n",
        "    g = grad(t)\n",
        "    norm = np.linalg.norm(g)\n",
        "    d = -g/norm\n",
        "    t_seq = [t]\n",
        "    d_seq = [d]\n",
        "\n",
        "    for _ in range(niter):\n",
        "        alpha *= gamma\n",
        "        t = t + alpha*d\n",
        "        t_seq.append(t)\n",
        "        \n",
        "        g = grad(t)\n",
        "        norm = np.linalg.norm(g)\n",
        "        d = -g/norm\n",
        "        d_seq.append(d)\n",
        "        \n",
        "    t_seq = np.array(t_seq)\n",
        "    d_seq = np.array(d_seq)\n",
        "    return t_seq, d_seq\n",
        "\n",
        "def MomentumGradientDescent(f, grad, t, alpha, gamma, beta, niter=100000):\n",
        "    v = np.zeros(t.shape[0])\n",
        "    t_seq = [t]\n",
        "    v_seq = [v]\n",
        "\n",
        "    for _ in range(niter):\n",
        "        g = grad(t)\n",
        "        norm = np.linalg.norm(g)\n",
        "        v = beta*v - alpha*g/norm\n",
        "        v_seq.append(v)\n",
        "\n",
        "        alpha *= gamma\n",
        "        t = t + v\n",
        "        t_seq.append(t)\n",
        "\n",
        "    t_seq = np.array(t_seq)\n",
        "    v_seq = np.array(v_seq)\n",
        "    return t_seq, v_seq  \n",
        "\n",
        "def NesterovMomentumGradientDescent(f, grad, t, alpha, gamma, beta, niter=100000):\n",
        "    g = grad(t)\n",
        "    norm = np.linalg.norm(g)\n",
        "    v = np.zeros(t.shape[0])\n",
        "    t_seq = [t]\n",
        "    v_seq = [v]\n",
        "    \n",
        "\n",
        "    for _ in range(niter):\n",
        "        g = grad(t + beta*v)\n",
        "        norm = np.linalg.norm(g)\n",
        "        v = beta*v - alpha*g/norm\n",
        "        v_seq.append(v)\n",
        "        \n",
        "        alpha *= gamma\n",
        "        t = t + v\n",
        "        t_seq.append(t)\n",
        "        \n",
        "    t_seq = np.array(t_seq)\n",
        "    v_seq = np.array(v_seq)\n",
        "    return t_seq, v_seq \n",
        "\n"
      ],
      "metadata": {
        "id": "uWhKw1lv0qZy"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialization\n",
        "alpha_init = 0.2 \n",
        "gamma = 0.9\n",
        "beta = 0.8\n",
        "t_init = np.array([-1.25, 0.5]) \n",
        "niter = 100000 \n",
        "\n",
        "t1_range = np.arange(-1.5, 1.75, 0.01)\n",
        "t2_range = np.arange(-0.5, 1.5, 0.01)\n",
        "\n",
        "A = np.meshgrid(t1_range, t2_range)\n",
        "Z = f(A)\n"
      ],
      "metadata": {
        "id": "ICkrWhBr04xM"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t_seq_gd, d_seq_gd = GradientDescent(f, grad, t_init, alpha_init, gamma, niter)\n",
        "t_seq_mgd, d_seq_mgd = MomentumGradientDescent(f, grad, t_init, alpha_init, gamma, beta, niter)\n",
        "t_seq_nmgd, d_seq_nmgd = NesterovMomentumGradientDescent(f, grad, t_init, alpha_init, gamma, beta, niter)\n"
      ],
      "metadata": {
        "id": "HBbtG0r105yh"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "plt.contour(t1_range, t2_range, Z, levels=np.exp(np.arange(-10, 6, 0.5)), cmap='Blues')\n",
        "plt.plot(t_seq_gd[:, 0], t_seq_gd[:, 1], '-b', label='GD')\n",
        "plt.plot(t_seq_mgd[:, 0], t_seq_mgd[:, 1], '-r', label='MGD')\n",
        "plt.plot(t_seq_nmgd[:, 0], t_seq_nmgd[:, 1], '-g', label='NMGD')\n",
        "plt.plot(1, 1, 'oy')\n",
        "plt.xlabel(r'$\\theta_1$')\n",
        "plt.ylabel(r'$\\theta_2$')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "H6vm2skz1DXn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "-auhck1l1KW1"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "imA6EsC85FJ6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}