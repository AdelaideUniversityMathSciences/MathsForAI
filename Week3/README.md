# Week 3

### Pre-reading

**Sampling**

+ "Communication in the Presence of Noise," C.E. Shannon, 1949, http://fab.cba.mit.edu/classes/S62.12/docs/Shannon_noise.pdfÂ or https://web.archive.org/web/20100208112344/http://www.stanford.edu/class/ee104/shannonpaper.pdf

**Fourier transforms**

+ https://www.cv.nrao.edu/~sransom/web/A1.html
+ https://ethz.ch/content/dam/ethz/special-interest/baug/ibk/structural-mechanics-dam/education/identmeth/fourier.pdf

**Entropy and information theory**

+ "A Mathematical Theory of Communication," C.E. Shannon, 1948, https://people.math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf
+ https://people.math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf
+ https://towardsdatascience.com/must-know-information-theory-concepts-in-deep-learning-ai-e54a5da9769d

### Topics

1. Signals and Sampling (and a mention of noise)

2. Bases, Transformation, the DFT, Symmetry and Invariance

3. Convolutions and filtering (and a mention of noise)

### Code

As before Code in that directory of the GitHub

+ `shannon_interp.ipynb`
+ `fft_example.ipynb`
+ `audio_example.ipynb`
+ `Noise_example.ipynb`
