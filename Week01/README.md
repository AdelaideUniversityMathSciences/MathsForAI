# Week 1
## Introduction to AI (Simon Lucey)

This week covers a general course introduction, an introduction to AI and its mathematical roots, plus an intro to using PyTorch in Google Colab.

### Resources & Readings

Papers mentioned in the slides and other readings/resources related to this week's content:

- Rosenblatt, F. (1958). [The perceptron: A probabilistic model for information storage and organization in the brain](https://doi.org/10.1037/h0042519). *Psychological Review*, 65(6), 386–408. 
- Minksy (1969). [Perceptrons](https://archive.org/details/perceptronsintro00mins) book.
- Vaswani *et al* (2017). [Attention is all you need](https://arxiv.org/abs/1706.03762): The foundational paper for transformer models.
- Olshausen & Field (1996). [Emergence of simple-cell receptive field properties by learning a sparse code for natural images](https://www.nature.com/articles/381607a0). *Nature* 381, pages 607–609: early models of human vision.
- Kingdom, Field & Olmos (2007). [Does spatial invariance result from insensitivity to change?](https://doi.org/10.1167/7.14.11) *Journal of Vision* July 2016, Vol.7, 11: paper on the difference between geometric versus Gaussian noise.
- Krizhevsky, Sutskever & Hinton (2012). [ImageNet Classification with Deep Convolutional Neural Networks](https://proceedings.neurips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html): AlexNet, the foundational paper for deep learning in computer vision.
- Hoshen & Peleg (2016). [Visual Learning of Arithmetic Operation](https://ojs.aaai.org/index.php/AAAI/article/view/9882). *Proceedings of the AAAI Conference on Artificial Intelligence*, 30(1). 
- Dziri *et al* (2023). [Faith and Fate: Limits of Transformers on Compositionality](https://arxiv.org/abs/2305.18654): Illustrates that even GPT-4 can't do arithmetic well, and offers an explanation why not.
- Zhang *et al* (2021). [Understanding deep learning (still) requires rethinking generalization
](https://dl.acm.org/doi/abs/10.1145/3446776). *Communications of the ACM*, Volume 64, Issue 3, pp 107-115.

Some other related goodies:

- On "GPT self-cannibalisation": [The Curse of Recursion: Training on Generated Data Makes Models Forget](https://arxiv.org/abs/2305.17493) (2023)
- Geoffrey Hinton, the "[Godfather of AI](https://www.theguardian.com/technology/2023/may/05/geoffrey-hinton-godfather-of-ai-fears-for-humanity)", on [how neural networks revolutionized AI](https://www.wnycstudios.org/podcasts/otm/segments/how-neural-networks-revolutionized-ai-on-the-media): good podcast describing the history of NNs.

