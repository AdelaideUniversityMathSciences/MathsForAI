# Week 7

### Topics

1. Challenges of optimisation for neural nets
2. Advanced optimisation algorithms: Momentum, Nesterov momentum, adaptive gradient methods
3. BatchNorm (if time, probably not)

We'll refer to and play around with the [compareOptimisationAlgos.ipynb](https://github.com/AdelaideUniversityMathSciences/MathsForAI/blob/main/Code/compareOptimisationAlgos.ipynb) notebook in the Code directory of this repo.

### Links
- I inserted some of Georg Gottwald's week 3 slides from [Machine Learning for the Working Mathematician](https://sites.google.com/view/mlwm-seminar-2022) into this week's material.
- Roger Grosse's course on *Neural Net Training Dynamics at the University of Toronto, CSC2541*: https://www.cs.toronto.edu/~rgrosse/courses/csc2541_2021/ I borrowed some of the explanation of momentum from this.
- Gabriel Goh, *Why momentum really works*: https://distill.pub/2017/momentum/
- Fabian Pedregosa, *A birds-eye view of optimization algorithms*: https://fa.bianp.net/teaching/2018/eecs227at/
- Benoit Liquet, Sarat Moka, and Yoni Nazarathy, *The Mathematical Engineering of Deep Learning*, Chapter 3: https://deeplearningmath.org/optimization-algorithms.html 
- For those who are interested in reading (much) more about *Continuous-time analysis of momentum methods*, check out this 2021 JMLR paper by Kovachki & Stuart: https://jmlr.org/papers/v22/19-466.html 
