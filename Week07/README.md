# Week 7

### Topics

1. Challenges of optimisation for neural nets
2. Advanced optimisation algorithms: Momentum, Nesterov momentum, adaptive gradient methods
3. BatchNorm (if time, probably not)

We'll refer to and play around with the compareOptimisationAlgos.ipynb in the Code directory of this repo.

### Links
- Roger Grosse's course on *Neural Net Training Dynamics at the University of Toronto, CSC2541*: https://www.cs.toronto.edu/~rgrosse/courses/csc2541_2021/ I borrowed some of the explanation of momentum from this
- Gabriel Goh, *Why momentum really works*: https://distill.pub/2017/momentum/
- Fabian Pedregosa, *A birds-eye view of optimization algorithms*: https://fa.bianp.net/teaching/2018/eecs227at/
- Benoit Liquet, Sarat Moka, and Yoni Nazarathy, *The Mathematical Engineering of Deep Learning*, Chapter 3: https://deeplearningmath.org/optimization-algorithms.html 
